---
title: "Intro to LLMs"
author: "Himanshu"
date: "2024-09-27"
image: images/llm.jpg
---

This is a post about LLMs. Summary from Andrej Karpathy's talk "Intro to Large Language Models".

## What is a Large Language Model?

Large Language Models (LLMs) are a type of machine learning model that are designed to understand and generate human language. They are typically trained on massive amounts of text data, and they use this training to learn the patterns and structures of language.

LLMs are used for a wide range of tasks, including language translation, text summarization, question answering, and more. They are also used in chatbots, virtual assistants, and other applications that require natural language processing.

## Raw Notes
- 2 files
    - parameters file
    - code file that runs the parameters ~ neural network architecture

Model Inference: running the model with a given set of parameters on a particular input
- input: string
- output: string

Model Training: updating the parameters of the model to improve its performance on a particular task
- input: string
- output: string

Neural Network predicts the next token in the sequence.
    - token: a word or character in the text

Pretraining(self-supervised learning):
- large corpus of text data
- feed the text into the neural network
- update the parameters of the neural network
- repeat for many epochs
- happens once a year because it's very expensive
- compresses the data into a neural network 

Finetuning (alighnment):
- smaller dataset
- faster to train
- better for a specific task
- happens once a month/week

Stage 3 - RLHF (Reinforcement Learning from Human Feedback)
- Supervised Learning
    - human writes a prompt
    - model writes a response
    - human judges the response
    - human provides feedback to the model
    - model updates the parameters
    
    

## How to make a LLM?

1. Collect a corpus of text data.
2. Preprocess the data into tokens.
3. Create a vocabulary of tokens.
4. Train a neural network to predict the next token in the sequence.
5. Use the trained model to generate text.
