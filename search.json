[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Musings",
    "section": "",
    "text": "Intro to LLMs\n\n\n\n\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nHimanshu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Himanshu",
    "section": "",
    "text": "Hey there! I’m Himanshu—a curious computer science grad student navigating the wonderful chaos of code, data, and the occasional tennis ball. Welcome to my little corner of the internet!\nThis is where I spill the tea (and maybe some coffee) on the cool stuff I’m learning and building. Right now, I’m neck-deep in classes like AR, AI & ML in Financial Analytics, Advanced Database Systems, and Full Stack Deep Learning—because, why not juggle four challenges at once? Oh, and I’m slowly but surely making my way through the Deep Learning book. (Spoiler: It’s a journey, not a sprint.)\nWhen I’m not debugging code or brainstorming projects, you’ll probably find me lifting at the gym, experimenting in the kitchen with some decent (sometimes spectacular!) Indian dishes, or trying to perfect my tennis backhand—weather and time permitting. I’m all about finding a balance between brainy and brawny.\nSo, grab a cup of chai (or coffee, no judgment), and join me as I blog about my adventures in tech, the occasional life hack, and maybe even a recipe or two. Let’s make this a fun and inspiring ride!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Himanshu",
    "section": "Education",
    "text": "Education\nUniversity of Arkansas | Fayetteville, AR MS in Computer Science | August 2024 - May 2026\nVirginia Tech | Blacksburg, VA BS in Computer Science | August 2017 - May 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Himanshu",
    "section": "Experience",
    "text": "Experience\nUniversity of Arkansas | Fayetteville, AR Graduate Assistant - Data Analyst | August 2024 - Present\nDematic | Grand Rapids, MI Software Engineer | October 2021 - July 2024"
  },
  {
    "objectID": "posts/intro-to-llm/index.html",
    "href": "posts/intro-to-llm/index.html",
    "title": "Intro to LLMs",
    "section": "",
    "text": "This is a post about LLMs. Summary from Andrej Karpathy’s talk “Intro to Large Language Models”."
  },
  {
    "objectID": "posts/intro-to-llm/index.html#what-is-a-large-language-model",
    "href": "posts/intro-to-llm/index.html#what-is-a-large-language-model",
    "title": "Intro to LLMs",
    "section": "What is a Large Language Model?",
    "text": "What is a Large Language Model?\nLarge Language Models (LLMs) are a type of machine learning model that are designed to understand and generate human language. They are typically trained on massive amounts of text data, and they use this training to learn the patterns and structures of language.\nLLMs are used for a wide range of tasks, including language translation, text summarization, question answering, and more. They are also used in chatbots, virtual assistants, and other applications that require natural language processing."
  },
  {
    "objectID": "posts/intro-to-llm/index.html#raw-notes",
    "href": "posts/intro-to-llm/index.html#raw-notes",
    "title": "Intro to LLMs",
    "section": "Raw Notes",
    "text": "Raw Notes\n\n2 files\n\nparameters file\ncode file that runs the parameters ~ neural network architecture\n\n\nModel Inference: running the model with a given set of parameters on a particular input - input: string - output: string\nModel Training: updating the parameters of the model to improve its performance on a particular task - input: string - output: string\nNeural Network predicts the next token in the sequence. - token: a word or character in the text\nPretraining(self-supervised learning): - large corpus of text data - feed the text into the neural network - update the parameters of the neural network - repeat for many epochs - happens once a year because it’s very expensive - compresses the data into a neural network\nFinetuning (alighnment): - smaller dataset - faster to train - better for a specific task - happens once a month/week\nStage 3 - RLHF (Reinforcement Learning from Human Feedback) - Supervised Learning - human writes a prompt - model writes a response - human judges the response - human provides feedback to the model - model updates the parameters"
  },
  {
    "objectID": "posts/intro-to-llm/index.html#how-to-make-a-llm",
    "href": "posts/intro-to-llm/index.html#how-to-make-a-llm",
    "title": "Intro to LLMs",
    "section": "How to make a LLM?",
    "text": "How to make a LLM?\n\nCollect a corpus of text data.\nPreprocess the data into tokens.\nCreate a vocabulary of tokens.\nTrain a neural network to predict the next token in the sequence.\nUse the trained model to generate text."
  }
]